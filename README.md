# AWS Weather Data Pipeline

Este projeto Ã© um **pipeline de dados na AWS** que coleta informaÃ§Ãµes climÃ¡ticas da [Open-Meteo API](https://open-meteo.com/), processa e enriquece os dados em diferentes camadas, armazenando-os de forma estruturada no **Amazon S3**.  

O objetivo Ã© construir um **data lake em camadas (raw, processed, enriched)**, aplicando boas prÃ¡ticas de engenharia de dados com Python, Polars e Wrangler, alÃ©m de integrar modelo de IA para enriquecimento de dados.

---

## ğŸ› ï¸ Arquitetura do Projeto

O pipeline consiste nas seguintes camadas:  

1. **Raw**
   - Coleta os dados da API Open-Meteo via `requests`.  
   - Armazena em JSON no S3, particionando por **data de extraÃ§Ã£o**.  

2. **Processed**
   - Converte os arquivos JSON em DataFrames com **Polars**.  
   - Separa um Ãºnico dataset em **trÃªs tabelas distintas**.  
   - Salva em formato **Parquet**, particionado por **ano/mÃªs/dia**.  

3. **Enriched**
   - Analisa uma das tabelas processadas.  
   - Utiliza o modelo do DeepSeek via **OpenRouter** para gerar descriÃ§Ãµes textuais do clima (ex: "Dia quente com alta umidade").  
   - Armazena novamente em Parquet, particionado por **ano/mÃªs/dia** e **estado**.  

---

## ğŸ“‚ Estrutura de Pastas

```bash
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements-test.txt
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ common
â”‚   â”‚   â”œâ”€â”€ configs
â”‚   â”‚   â”‚   â””â”€â”€ env_vars.py         # VariÃ¡veis de ambiente
â”‚   â”‚   â”œâ”€â”€ local.env               # Template de configuraÃ§Ã£o local
â”‚   â”‚   â””â”€â”€ services
â”‚   â”‚       â”œâ”€â”€ api.py              # ConexÃ£o com APIs externas
â”‚   â”‚       â””â”€â”€ s3.py               # FunÃ§Ãµes utilitÃ¡rias para S3
â”‚   â”œâ”€â”€ enrichment
â”‚   â”‚   â”œâ”€â”€ etl
â”‚   â”‚   â”‚   â”œâ”€â”€ extract.py          # ExtraÃ§Ã£o de dados da camada processed
â”‚   â”‚   â”‚   â”œâ”€â”€ load.py             # Carregamento para enriched
â”‚   â”‚   â”‚   â””â”€â”€ transform.py        # Enriquecimento com IA
â”‚   â”‚   â”œâ”€â”€ job.py
â”‚   â”‚   â””â”€â”€ lambda_function.py
â”‚   â”œâ”€â”€ ingestion
â”‚   â”‚   â”œâ”€â”€ data
â”‚   â”‚   â”‚   â””â”€â”€ states_mapping.py   # Mapeamento de lat/long das capitais brasileiras
â”‚   â”‚   â”œâ”€â”€ job.py
â”‚   â”‚   â””â”€â”€ lambda_function.py
â”‚   â””â”€â”€ processing
â”‚       â”œâ”€â”€ etl
â”‚       â”‚   â”œâ”€â”€ extract.py          # ExtraÃ§Ã£o da camada raw
â”‚       â”‚   â”œâ”€â”€ load.py             # Carregamento para processed
â”‚       â”‚   â””â”€â”€ transform.py        # Explode JSON e aplica schema
â”‚       â”œâ”€â”€ schema
â”‚       â”‚   â””â”€â”€ column_mapping.py   # Tipagem e normalizaÃ§Ã£o de colunas
â”‚       â”œâ”€â”€ job.py
â”‚       â””â”€â”€ lambda_function.py
â””â”€â”€ test
    â””â”€â”€ __init__.py                 # Testes unitÃ¡rios

```
---

## ğŸ’» Tecnologias Utilizadas

- **Linguagem**: Python (versÃ£o 3.10)
- **Bibliotecas principais**:  
  - [Polars](https://pola-rs.github.io/polars/) â†’ manipulaÃ§Ã£o de DataFrames  
  - [AWS Wrangler](https://github.com/awslabs/aws-data-wrangler) â†’ integraÃ§Ã£o com S3
  - [Requests](https://docs.python-requests.org/) â†’ coleta de dados da API  
  - [OpenRouter](https://openrouter.ai/) â†’ enriquecimento de dados com modelos de IA  

---

## âœ… PrÃ³ximos Passos

- [ ] Criar **Infraestrutura como CÃ³digo (IaC)** com Terraform.  
- [ ] Adicionar **testes unitÃ¡rios** em `test/`.  
